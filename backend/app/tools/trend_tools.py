"""
íŠ¸ë Œë“œ ë¶„ì„ ë„êµ¬
Naver DataLab APIë¥¼ í™œìš©í•œ ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„
"""
import json
import logging
import os
import re
import statistics
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import requests

from app.tools.llm import call_llm_with_context

logger = logging.getLogger(__name__)

NAVER_CLIENT_ID = os.getenv("NAVER_DATALAB_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_DATALAB_CLIENT_SECRET")
NAVER_DATALAB_URL = os.getenv(
    "NAVER_DATALAB_URL",
    "https://openapi.naver.com/v1/datalab/search",
)

STOPWORDS = {
    "ìµœê·¼",
    "ìš”ì¦˜",
    "ì§€ë‚œ",
    "ì´ë²ˆ",
    "ë‹¤ìŒ",
    "ê°œì›”",
    "ë‹¬",
    "ì›”",
    "ì£¼",
    "ì¼",
    "ë…„",
    "ì£¼ê°„",
    "ê°œì›”ê°„",
    "ë¶„ê¸°",
    "ë°˜ë…„",
    "íŠ¸ë Œë“œ",
    "trend",
    "ë¶„ì„",
    "ì•Œë ¤ì¤˜",
    "í•´ì£¼ì„¸ìš”",
    "í•´ì¤˜",
    "ë°ì´í„°",
    "ì‹œì¥",
    "ì–´ë–»ê²Œ",
    "ìš”ì²­",
    "ë³´ê³ ",
    "ì •ë³´",
    "ê´€ë ¨",
    "ëŒ€í•œ",
    "ì…ë‹ˆë‹¤",
    "ì£¼ì„¸ìš”",
    "please",
    "tell",
    "show",
    "about",
    "analysis",
    "ì •ë¦¬",
    "í•´ì¤˜ìš”",
    "ì•Œë ¤ì£¼ì„¸ìš”",
}


def extract_trend_keyword(user_message: str, fallback_to_llm: bool = True) -> Optional[str]:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•œë‹¤."""
    if not user_message:
        return None

    text = user_message.strip()
    if not text:
        return None

    logger.info("í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘: '%s'", text)

    quoted_matches = re.findall(r"[\"""''']([^\"""''']{2,})[\"""''']", text)
    for candidate in quoted_matches:
        cleaned = _clean_keyword(candidate)
        if cleaned:
            logger.info("ë”°ì˜´í‘œ íŒ¨í„´ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ: '%s' -> '%s'", candidate, cleaned)
            return cleaned

    hashtags = re.findall(r"#([A-Za-z0-9ê°€-í£]+)", text)
    if hashtags:
        cleaned = _clean_keyword(hashtags[0])
        if cleaned:
            logger.info("í•´ì‹œíƒœê·¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ: '%s' -> '%s'", hashtags[0], cleaned)
            return cleaned

    # íŒ¨í„´ ë§¤ì¹­ ì „ì— ê¸°ê°„ í‘œí˜„ ì œê±°í•œ ë²„ì „ìœ¼ë¡œ ì‹œë„
    text_without_period = re.sub(
        r"(?:ìµœê·¼|ìš”ì¦˜|ì§€ë‚œ|ì´ë²ˆ|ë‹¤ìŒ)\s*(?:\d+\s*)?(?:ë…„|ê°œì›”|ë‹¬|ì›”|ì£¼|ì¼|ì£¼ê°„|ê°œì›”ê°„|ë¶„ê¸°|ë°˜ë…„)?",
        "",
        text,
        flags=re.IGNORECASE
    )
    logger.debug("ê¸°ê°„ í‘œí˜„ ì œê±° í›„: '%s'", text_without_period)

    patterns = (
        r"(?P<keyword>[ê°€-í£A-Za-z0-9&\s]+?)\s*(?:íŠ¸ë Œë“œ|trend)\s*(?:ë¶„ì„|ì•Œë ¤ì¤˜|ë°ì´í„°|í˜„í™©|ë³´ê³ |íŒŒì•…)?",
        r"(?P<keyword>[ê°€-í£A-Za-z0-9&\s]+?)\s*(?:ì‹œì¥|ìˆ˜ìš”)\s*(?:ì „ë§|ë¶„ì„|ì–´ë–»ê²Œ|ì¶”ì´)",
        r"(?P<keyword>[ê°€-í£A-Za-z0-9&\s]+?)\s*(?:ì— ëŒ€í•œ|ê´€ë ¨)\s*(?:íŠ¸ë Œë“œ|ë¶„ì„)",
    )
    for i, pattern in enumerate(patterns):
        match = re.search(pattern, text_without_period, re.IGNORECASE)
        if match:
            raw_keyword = match.group("keyword")
            cleaned = _clean_keyword(raw_keyword)
            if cleaned:
                logger.info("íŒ¨í„´ %dì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ: '%s' -> '%s'", i+1, raw_keyword, cleaned)
                return cleaned

    tokens = [token for token in re.split(r"[,\s]+", text) if token]
    filtered = [token for token in tokens if _is_meaningful_token(token)]
    if filtered:
        candidate = " ".join(filtered[:2]) if len(
            filtered) > 1 else filtered[0]
        cleaned = _clean_keyword(candidate)
        if cleaned:
            logger.info("í† í° ë¶„ì„ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ: '%s' -> '%s'", candidate, cleaned)
            return cleaned

    if fallback_to_llm:
        logger.info("LLM í´ë°±ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„")
        result = _extract_keyword_with_llm(text)
        if result:
            logger.info("LLMì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ: '%s'", result)
        return result

    logger.warning("í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
    return None


def resolve_time_window(user_message: str) -> Dict[str, Any]:
    """ì‚¬ìš©ì ë¬¸ì¥ì—ì„œ ë¶„ì„ ê¸°ê°„ì„ ì¶”ì •í•œë‹¤."""
    # í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ëŠ˜ ë‚ ì§œ ê³„ì‚° (UTC+9)
    from datetime import timezone, timedelta as td
    kst = timezone(td(hours=9))
    now = datetime.now(kst).replace(tzinfo=None)

    # ë„¤ì´ë²„ DataLabì€ ì–´ì œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì œê³µí•˜ë¯€ë¡œ end_dateë¥¼ ì–´ì œë¡œ ì„¤ì •
    yesterday = now - timedelta(days=1)

    default_days = 180
    default = {
        "start_date": (yesterday - timedelta(days=default_days)).strftime("%Y-%m-%d"),
        "end_date": yesterday.strftime("%Y-%m-%d"),
        "time_unit": "week",
        "days": default_days,
    }

    if not user_message:
        return default

    text = user_message.lower()
    days: Optional[int] = None

    match = re.search(r"(\d+)\s*(?:ì¼|ì¼ê°„|ì¼ë™ì•ˆ|days?)", text)
    if match:
        days = max(1, int(match.group(1)))

    if days is None:
        match = re.search(r"(\d+)\s*(?:ì£¼|ì£¼ê°„|weeks?)", text)
        if match:
            days = int(match.group(1)) * 7

    if days is None:
        match = re.search(r"(\d+)\s*(?:ê°œì›”|ë‹¬|months?)", text)
        if match:
            days = int(match.group(1)) * 30

    if days is None:
        match = re.search(r"(\d+)\s*(?:ë…„|years?)", text)
        if match:
            days = int(match.group(1)) * 365

    if days is None:
        condensed = text.replace(" ", "")
        if "ë¶„ê¸°" in text:
            days = 90
        elif "ë°˜ë…„" in text:
            days = 180
        elif "1ë…„" in condensed or "ì¼ë…„" in condensed:
            days = 365
        elif "3ë…„" in condensed:
            days = 365 * 3
        elif "5ë…„" in condensed:
            days = 365 * 5

    if days is None:
        return default

    days = max(7, min(days, 365 * 10))
    start = (yesterday - timedelta(days=days)).strftime("%Y-%m-%d")
    time_unit = "date"
    if days > 120:
        time_unit = "week"
    if days > 365 * 2:
        time_unit = "month"

    return {
        "start_date": start,
        "end_date": yesterday.strftime("%Y-%m-%d"),
        "time_unit": time_unit,
        "days": days,
    }


def get_naver_datalab_trends(
    keywords: List[str],
    start_date: str,
    end_date: str,
    time_unit: str = "date",
) -> Dict[str, Any]:
    """Naver DataLab APIë¡œ íŠ¸ë Œë“œ ì¡°íšŒ (ì‹¤íŒ¨ ì‹œ ëª¨ì˜ ë°ì´í„°)."""
    logger.info("Naver DataLab ì¡°íšŒ: %s (%s~%s)", keywords, start_date, end_date)
    logger.info("API ì¸ì¦ ìƒíƒœ - Client ID: %s, Secret: %s",
                "ì„¤ì •ë¨" if NAVER_CLIENT_ID else "ì—†ìŒ",
                "ì„¤ì •ë¨" if NAVER_CLIENT_SECRET else "ì—†ìŒ")

    if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
        payload = {
            "startDate": start_date,
            "endDate": end_date,
            "timeUnit": time_unit,
            "keywordGroups": [
                {"groupName": kw, "keywords": [kw]}
                for kw in keywords[:5]
            ],
            "device": "",
            "gender": "",
            "ages": [],
        }
        try:
            response = requests.post(
                NAVER_DATALAB_URL,
                headers={
                    "X-Naver-Client-Id": NAVER_CLIENT_ID,
                    "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
                    "Content-Type": "application/json",
                },
                json=payload,
                timeout=10,
            )
            response.raise_for_status()
            data = response.json()
            logger.info("Naver DataLab API ì‘ë‹µ: %s", json.dumps(data, ensure_ascii=False)[:500])
            results = data.get("results")
            if results:
                logger.info("API ê²°ê³¼ ìˆ˜: %dê°œ", len(results))
                normalized: Dict[str, Any] = {
                    "keywords": keywords[:5],
                    "period": {"start": start_date, "end": end_date},
                    "time_unit": time_unit,
                    "results": [],
                    "is_mock": False,
                }
                for entry in results:
                    series = [
                        {"date": item.get("period"),
                         "value": item.get("ratio")}
                        for item in entry.get("data", [])
                        if item.get("ratio") is not None
                    ]
                    normalized["results"].append(
                        {
                            "group": entry.get("title") or entry.get("groupName") or keywords[0],
                            "keywords": entry.get("keywords", []),
                            "series": series,
                        }
                    )
                return normalized
            else:
                logger.warning("Naver DataLab API ì‘ë‹µì— results í•„ë“œê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŒ")
        except requests.RequestException as exc:
            logger.warning("Naver DataLab API í˜¸ì¶œ ì‹¤íŒ¨: %s", exc)
        except ValueError as exc:
            logger.warning("Naver DataLab ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: %s", exc)
    else:
        logger.info("Naver DataLab API í‚¤ ë¯¸ì„¤ì • - ëª¨ì˜ ë°ì´í„° ì‚¬ìš©")

    return _generate_mock_naver_trends(keywords, start_date, end_date, time_unit)


def analyze_trend_data(trend_data: Dict[str, Any]) -> Dict[str, Any]:
    """ìˆ˜ì§‘í•œ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ í•µì‹¬ ì§€í‘œì™€ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•œë‹¤."""
    logger.info("íŠ¸ë Œë“œ ë°ì´í„° ë¶„ì„ ì‹œì‘")

    keyword = trend_data.get("keyword")
    start_date = trend_data.get("start_date")
    end_date = trend_data.get("end_date")
    time_unit = trend_data.get("time_unit")

    naver_data = trend_data.get("naver") or {}

    naver_series: List[Dict[str, Any]] = []
    if naver_data:
        if "results" in naver_data:
            for entry in naver_data["results"]:
                naver_series.extend(entry.get("series", []))
        elif "data" in naver_data:
            for item in naver_data["data"]:
                naver_series.append(
                    {"date": item.get("period"), "value": item.get("ratio")})

    naver_metrics = _compute_series_metrics(naver_series)

    if naver_metrics["has_data"]:
        summary_lines = [
            f"- ê²€ìƒ‰ ì§€ìˆ˜ {naver_metrics['momentum_label']} íë¦„ (ìµœê·¼ ë³€í™” {format_percentage(naver_metrics['momentum_pct'])})",
            f"- ì²« ì‹œì  ëŒ€ë¹„ ë³€í™”ìœ¨ {format_percentage(naver_metrics['growth_pct'])}",
        ]
    else:
        summary_lines = ["- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]

    signal = _infer_signal(naver_metrics)

    clusters = generate_keyword_clusters(
        keyword=keyword,
        summary_lines=summary_lines.copy(),
        naver_metrics=naver_metrics,
        time_unit=time_unit,
        start_date=start_date,
        end_date=end_date,
        naver_data=naver_data,
    )

    cluster_summary = None
    if clusters:
        cluster_summary = ", ".join(
            f"{cluster.get('name', 'í´ëŸ¬ìŠ¤í„°')} {format_percentage(cluster.get('change_pct'))}"
            for cluster in clusters[:3]
        )
        summary_lines.append(f"- ì—°ê´€ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°: {cluster_summary}")

    insight = _generate_insights_with_llm(
        keyword,
        summary_lines,
        naver_metrics,
        time_unit,
        start_date,
        end_date,
    )
    if not insight:
        insight = _generate_rule_based_insight(keyword, naver_metrics)

    confidence = "ë†’ìŒ" if naver_metrics["has_data"] and not naver_data.get("is_mock", True) else (
        "ì¤‘ê°„" if naver_metrics["has_data"] else "ë‚®ìŒ"
    )

    return {
        "keyword": keyword,
        "start_date": start_date,
        "end_date": end_date,
        "time_unit": time_unit,
        "naver": {
            **naver_metrics,
            "time_unit": naver_data.get("time_unit"),
            "is_mock": naver_data.get("is_mock", True),
        },
        "summary": "\n".join(summary_lines),
        "insight": insight,
        "signal": signal,
        "confidence": confidence,
        "clusters": clusters,
        "cluster_summary": cluster_summary,
    }


def format_percentage(value: Optional[float]) -> str:
    if isinstance(value, (int, float)):
        return f"{value:+.1f}%"
    return "N/A"


def _normalize_whitespace(value: str) -> str:
    return re.sub(r"\s+", " ", value.strip())


def _clean_keyword(keyword: str) -> Optional[str]:
    if not keyword:
        return None
    cleaned = _normalize_whitespace(keyword)

    # ê¸°ê°„ ê´€ë ¨ í‘œí˜„ ì œê±° (ë§¤ìš° ì¤‘ìš”!)
    cleaned = re.sub(r"\s*(?:ìµœê·¼|ìš”ì¦˜|ì§€ë‚œ|ì´ë²ˆ|ë‹¤ìŒ)\s*(?:\d+\s*)?(?:ë…„|ê°œì›”|ë‹¬|ì›”|ì£¼|ì¼|ì£¼ê°„|ê°œì›”ê°„|ë¶„ê¸°|ë°˜ë…„)?", "", cleaned, flags=re.IGNORECASE)

    # ë¶„ì„/íŠ¸ë Œë“œ ê´€ë ¨ ë‹¨ì–´ ì œê±°
    cleaned = re.sub(r"(?:íŠ¸ë Œë“œ|trend|ë¶„ì„|ì‹œì¥|ë°ì´í„°|ì „ë§|ì¶”ì´|í˜„í™©|ë³´ê³ |íŒŒì•…)(?:\s+|$)", "", cleaned, flags=re.IGNORECASE)

    # ì ‘ë‘ì–´ ì œê±°
    cleaned = re.sub(r"^(?:ëŒ€í•œ|ê´€ë ¨|êµ­ë‚´|í•´ì™¸)\s+", "", cleaned)

    # í•œê¸€ ì¡°ì‚¬ ì œê±° (ì˜, ì„, ë¥¼, ì´, ê°€, ì€, ëŠ”, ì™€, ê³¼, ì—, ì—ì„œ, ìœ¼ë¡œ, ë¡œ)
    cleaned = re.sub(r"(ì˜|ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ì™€|ê³¼|ì—ì„œ|ìœ¼ë¡œ|ì—|ë¡œ)$", "", cleaned)

    # ê³µë°± ì •ë¦¬ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
    cleaned = cleaned.strip(' "\'()[]')

    if len(cleaned) < 2:
        return None
    if cleaned.lower() in STOPWORDS:
        return None
    return cleaned


def _is_meaningful_token(token: str) -> bool:
    stripped = token.strip(' "\'()[]{}.,!?')
    if not stripped:
        return False
    lower = stripped.lower()
    if lower in STOPWORDS:
        return False
    if "íŠ¸ë Œë“œ" in lower or "trend" in lower or "ë¶„ì„" in lower:
        return False
    if len(stripped) == 1 and not stripped.isdigit():
        return False
    return True


def _extract_keyword_with_llm(text: str) -> Optional[str]:
    try:
        response = call_llm_with_context(
            [
                {
                    "role": "system",
                    "content": (
                        "ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ë°ì´í„° ë¶„ì„ ë³´ì¡° ë„êµ¬ì…ë‹ˆë‹¤. "
                        "ì‚¬ìš©ìê°€ ìš”ì²­í•œ ë¬¸ì¥ì—ì„œ ë¶„ì„í•  í•µì‹¬ í‚¤ì›Œë“œ ë˜ëŠ” ì œí’ˆëª…ì„ í•œ ì¤„ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. "
                        "ë¶ˆí•„ìš”í•œ ì„¤ëª…ê³¼ ë”°ì˜´í‘œëŠ” ì œê±°í•˜ì„¸ìš”."
                    ),
                },
                {"role": "user", "content": text},
            ]
        )
        if response.get("success"):
            raw = response.get("reply_text", "").strip()
            if not raw:
                return None
            keyword = raw.splitlines()[0]
            return _clean_keyword(keyword)
    except Exception as exc:
        logger.debug("LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: %s", exc)
    return None


def _generate_mock_naver_trends(
    keywords: List[str],
    start_date: str,
    end_date: str,
    time_unit: str,
) -> Dict[str, Any]:
    logger.info("Naver DataLab ëª¨ì˜ ë°ì´í„° ì‚¬ìš©: %s", keywords)
    start_dt = _parse_date(start_date) or (
        datetime.utcnow() - timedelta(days=90))
    end_dt = _parse_date(end_date) or datetime.utcnow()
    if end_dt <= start_dt:
        end_dt = start_dt + timedelta(days=30)

    total_days = max(1, (end_dt - start_dt).days)
    if time_unit == "date":
        steps = max(8, min(20, total_days // 5 or 8))
        step_days = max(1, total_days // max(steps - 1, 1))
    elif time_unit == "week":
        steps = max(8, min(20, total_days // 7 or 8))
        step_days = 7
    else:
        steps = max(6, min(20, total_days // 30 or 6))
        step_days = 30

    results = []
    for idx, keyword in enumerate(keywords[:5]):
        series = []
        baseline = 40 + (hash((keyword, idx)) % 40)
        slope = (hash((keyword, "trend")) % 5) - 2
        for i in range(steps):
            dt = start_dt + timedelta(days=i * step_days)
            if dt > end_dt:
                dt = end_dt
            jitter = (hash((keyword, i)) % 10) - 5
            value = max(5, min(100, baseline + slope *
                        (i - steps // 2) + jitter))
            series.append({"date": dt.strftime("%Y-%m-%d"), "value": value})
        results.append(
            {
                "group": keyword,
                "keywords": [keyword],
                "series": series,
            }
        )

    return {
        "keywords": keywords[:5],
        "period": {"start": start_date, "end": end_date},
        "time_unit": time_unit,
        "results": results,
        "is_mock": True,
    }


def _compute_series_metrics(series: List[Dict[str, Any]]) -> Dict[str, Any]:
    if not series:
        return {
            "has_data": False,
            "data_points": 0,
            "average": None,
            "latest_value": None,
            "latest_date": None,
            "growth_pct": None,
            "momentum_pct": None,
            "momentum_label": "ë°ì´í„° ë¶€ì¡±",
            "peak": None,
            "volatility": None,
            "series_tail": [],
            "first_value": None,
        }

    cleaned = []
    for idx, point in enumerate(series):
        value = point.get("value")
        if value is None:
            continue
        try:
            numeric = float(value)
        except (TypeError, ValueError):
            continue
        date_obj = _parse_date(point.get("date"))
        cleaned.append((date_obj or (
            datetime.min + timedelta(days=idx)), idx, point.get("date"), numeric))

    if not cleaned:
        return {
            "has_data": False,
            "data_points": 0,
            "average": None,
            "latest_value": None,
            "latest_date": None,
            "growth_pct": None,
            "momentum_pct": None,
            "momentum_label": "ë°ì´í„° ë¶€ì¡±",
            "peak": None,
            "volatility": None,
            "series_tail": [],
            "first_value": None,
        }

    cleaned.sort(key=lambda item: (item[0], item[1]))
    ordered_dates = [item[2] for item in cleaned]
    values = [item[3] for item in cleaned]

    data_points = len(values)
    average = sum(values) / data_points
    first_value = values[0]
    latest_value = values[-1]
    growth_pct = ((latest_value - first_value) /
                  first_value * 100) if first_value else None

    window = min(3, data_points)
    early_avg = sum(values[:window]) / window if window else None
    recent_avg = sum(values[-window:]) / window if window else None
    momentum_pct = ((recent_avg - early_avg) /
                    early_avg * 100) if early_avg else None
    momentum_label = _momentum_label(momentum_pct)

    peak_index = max(range(data_points), key=lambda i: values[i])
    peak = {"date": ordered_dates[peak_index], "value": values[peak_index]}

    try:
        volatility = statistics.stdev(values) if data_points > 1 else 0.0
    except statistics.StatisticsError:
        volatility = 0.0

    series_tail = [
        {"date": ordered_dates[i], "value": values[i]}
        for i in range(max(0, data_points - 5), data_points)
    ]

    return {
        "has_data": True,
        "data_points": data_points,
        "average": average,
        "latest_value": latest_value,
        "latest_date": ordered_dates[-1],
        "growth_pct": growth_pct,
        "momentum_pct": momentum_pct,
        "momentum_label": momentum_label,
        "peak": peak,
        "volatility": volatility,
        "series_tail": series_tail,
        "first_value": first_value,
    }


def _parse_date(value: Optional[str]) -> Optional[datetime]:
    if value is None:
        return None
    if isinstance(value, datetime):
        return value
    try:
        return datetime.fromisoformat(str(value).replace("Z", "+00:00"))
    except ValueError:
        pass
    for fmt in ("%Y-%m-%d", "%Y%m%d"):
        try:
            return datetime.strptime(str(value), fmt)
        except ValueError:
            continue
    return None


def _momentum_label(momentum_pct: Optional[float]) -> str:
    if not isinstance(momentum_pct, (int, float)):
        return "ë°ì´í„° ë¶€ì¡±"
    if momentum_pct > 5:
        return "ìƒìŠ¹"
    if momentum_pct < -5:
        return "í•˜ë½"
    return "ë³´í•©"


def _infer_signal(metrics: Dict[str, Any]) -> str:
    if not metrics.get("has_data"):
        return "ë°ì´í„° ë¶€ì¡±"

    score = 0.0
    weight = 0.0

    growth = metrics.get("growth_pct")
    momentum = metrics.get("momentum_pct")
    latest = metrics.get("latest_value")

    if isinstance(growth, (int, float)):
        score += growth * 0.6
        weight += 0.6
    if isinstance(momentum, (int, float)):
        score += momentum * 0.4
        weight += 0.4
    if isinstance(latest, (int, float)) and latest >= 70:
        score += 5

    if weight == 0:
        return "ë°ì´í„° ë¶€ì¡±"

    normalized = score / weight
    if normalized >= 20:
        return "ğŸš€ ê°•í•œ ìƒìŠ¹ì„¸"
    if normalized >= 8:
        return "â†—ï¸ ì™„ë§Œí•œ ìƒìŠ¹"
    if normalized <= -20:
        return "ğŸ“‰ ê°•í•œ í•˜ë½"
    if normalized <= -8:
        return "â†˜ï¸ ì™„ë§Œí•œ í•˜ë½"
    return "â– ë³´í•©ì„¸"


def _generate_insights_with_llm(
    keyword: Optional[str],
    summary_lines: List[str],
    naver_metrics: Dict[str, Any],
    time_unit: Optional[str],
    start_date: Optional[str],
    end_date: Optional[str],
) -> Optional[str]:
    if not keyword:
        return None

    try:
        metrics_brief = {
            "naver": {
                "average": naver_metrics.get("average"),
                "growth_pct": naver_metrics.get("growth_pct"),
                "momentum_pct": naver_metrics.get("momentum_pct"),
                "latest_value": naver_metrics.get("latest_value"),
                "peak": naver_metrics.get("peak"),
            }
        }

        summary_text = "\n".join(summary_lines)

        response = call_llm_with_context(
            [
                {
                    "role": "system",
                    "content": (
                        "ë‹¹ì‹ ì€ ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµê°€ì…ë‹ˆë‹¤. Naver ê²€ìƒ‰ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ "
                        "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ì™€ ëŒ€ì‘ ì „ëµì„ ê°„ê²°í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”."
                    ),
                },
                {
                    "role": "user",
                    "content": (
                        f"í‚¤ì›Œë“œ: {keyword}\n"
                        f"ê¸°ê°„: {start_date} ~ {end_date} (ë‹¨ìœ„: {time_unit})\n"
                        f"ìš”ì•½: {summary_text}\n"
                        f"ì§€í‘œ: {json.dumps(metrics_brief, ensure_ascii=False)}\n"
                        "ì¦ê°€/ê°ì†Œ ì›ì¸ê³¼ ëŒ€ì‘ ì „ëµì„ bullet 2-3ê°œë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
                    ),
                },
            ]
        )

        if response.get("success"):
            return response.get("reply_text", "").strip() or None
    except Exception as exc:
        logger.debug("LLM ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: %s", exc)

    return None


def _generate_rule_based_insight(
    keyword: Optional[str],
    naver_metrics: Dict[str, Any],
) -> str:
    if not naver_metrics.get("has_data"):
        return "ìœ ì˜ë¯¸í•œ ê²€ìƒ‰ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ë„“í˜€ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ë³´ì„¸ìš”."

    lines: List[str] = []
    momentum_label = naver_metrics.get("momentum_label")
    momentum_pct = format_percentage(naver_metrics.get("momentum_pct"))
    growth_pct = format_percentage(naver_metrics.get("growth_pct"))

    lines.append(
        f"'{keyword}' ê²€ìƒ‰ ì§€ìˆ˜ëŠ” ìµœê·¼ {momentum_label} íë¦„ì´ë©° ë‹¨ê¸° ë³€í™”ìœ¨ì€ {momentum_pct}ì…ë‹ˆë‹¤."
    )
    lines.append(
        f"ë¶„ì„ ì‹œì‘ ì‹œì  ëŒ€ë¹„ ë³€í™”ìœ¨ì€ {growth_pct}ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ê²€ìƒ‰ í”¼í¬ ì‹œì ê³¼ ìµœì‹  ì§€ìˆ˜ë¥¼ ì°¸ê³ í•´ ì½˜í…ì¸  íƒ€ì´ë°ì„ ì¡°ì •í•˜ì„¸ìš”."
    )
    lines.append("ìƒìŠ¹ êµ¬ê°„ì—ì„œëŠ” ìº í˜ì¸ì„ í™•ëŒ€í•˜ê³ , í•˜ë½ êµ­ë©´ì—ì„œëŠ” ì—°ê´€ í‚¤ì›Œë“œë¥¼ ë°œêµ´í•´ ê´€ì‹¬ì„ ìœ ì§€í•˜ì„¸ìš”.")
    return "\n".join(lines)


def generate_keyword_clusters(
    keyword: Optional[str],
    summary_lines: List[str],
    naver_metrics: Dict[str, Any],
    time_unit: Optional[str],
    start_date: Optional[str],
    end_date: Optional[str],
    naver_data: Optional[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    if not keyword or not naver_metrics.get("has_data"):
        return []

    hints = _extract_related_terms(naver_data)
    hints = [term for term in dict.fromkeys(
        hints) if term and term.lower() != keyword.lower()]

    summary_text = "\n".join(summary_lines) if summary_lines else "ìš”ì•½ ì •ë³´ ì—†ìŒ"
    metrics_brief = {
        "momentum_pct": naver_metrics.get("momentum_pct"),
        "momentum_label": naver_metrics.get("momentum_label"),
        "growth_pct": naver_metrics.get("growth_pct"),
        "latest_value": naver_metrics.get("latest_value"),
    }

    try:
        hints_text = ", ".join(hints[:10]) if hints else "ì—†ìŒ"
        response = call_llm_with_context(
            [
                {
                    "role": "system",
                    "content": (
                        "ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê²€ìƒ‰ í‚¤ì›Œë“œì™€ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ "
                        "ì—°ê´€ í‚¤ì›Œë“œë¥¼ 3~5ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ê³  ê°ê°ì˜ ì¶”ì„¸ ë³€í™”ë¥¼ ë¶„ì„í•˜ì„¸ìš”. "
                        "JSON ë°°ì—´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ê° í•­ëª©ì€ name, keywords, trend_label, change_pct, insight í•„ë“œë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤."
                    ),
                },
                {
                    "role": "user",
                    "content": (
                        f"í‚¤ì›Œë“œ: {keyword}\n"
                        f"ê¸°ê°„: {start_date} ~ {end_date} (ë‹¨ìœ„: {time_unit})\n"
                        f"ìš”ì•½: {summary_text}\n"
                        f"ì§€í‘œ: {json.dumps(metrics_brief, ensure_ascii=False)}\n"
                        f"ì—°ê´€ íŒíŠ¸: {hints_text}\n"
                        "ì—°ê´€ í´ëŸ¬ìŠ¤í„°ë¥¼ 3~5ê°œ ì œì•ˆí•˜ê³ , ê° í´ëŸ¬ìŠ¤í„°ì˜ ìµœê·¼ ë³€í™”ìœ¨(change_pct)ì„ % ë‹¨ìœ„ì˜ ìˆ«ìë¡œ ì œê³µí•˜ì„¸ìš” (ì˜ˆ: 12.5ëŠ” +12.5%)."
                    ),
                },
            ]
        )

        if response.get("success"):
            clusters_raw = response.get("reply_text", "")
            json_str = _extract_json_block(clusters_raw)
            if json_str:
                data = json.loads(json_str)
                if isinstance(data, dict):
                    data = data.get("clusters") or data.get("result")
                if isinstance(data, list):
                    clusters = []
                    for item in data:
                        normalized = _normalize_cluster_entry(item)
                        if normalized:
                            clusters.append(normalized)
                    if clusters:
                        return clusters
    except Exception as exc:
        logger.debug("ì—°ê´€ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„° LLM ìƒì„± ì‹¤íŒ¨: %s", exc)

    return _create_cluster_fallback(keyword, hints, naver_metrics)


def _extract_related_terms(naver_data: Optional[Dict[str, Any]]) -> List[str]:
    terms: List[str] = []
    if not naver_data:
        return terms

    if "results" in naver_data:
        for entry in naver_data.get("results", []):
            for kw in entry.get("keywords", []):
                terms.append(kw)
    elif "data" in naver_data:
        for item in naver_data.get("data", []):
            kw = item.get("keyword") or item.get("title")
            if kw:
                terms.append(kw)

    return terms


def _extract_json_block(text: str) -> Optional[str]:
    if not text:
        return None
    match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
    if match:
        return match.group(1)
    bracket_match = re.search(r"\[\s*{.*}\s*\]", text, re.DOTALL)
    if bracket_match:
        return bracket_match.group(0)
    brace_match = re.search(r"{\s*\"clusters\"\s*:\s*\[.*\]}", text, re.DOTALL)
    if brace_match:
        return brace_match.group(0)
    return None


def _normalize_cluster_entry(entry: Any) -> Optional[Dict[str, Any]]:
    if not isinstance(entry, dict):
        return None

    name = entry.get("name") or entry.get("cluster")
    if not name:
        return None

    keywords = entry.get("keywords") or entry.get("terms") or []
    if isinstance(keywords, str):
        keywords = [kw.strip() for kw in keywords.split(',') if kw.strip()]

    trend_label = entry.get("trend_label") or entry.get(
        "trend") or entry.get("direction")
    change_pct = _parse_change_value(
        entry.get("change_pct") or entry.get("change") or entry.get("delta"))
    insight = entry.get("insight") or entry.get(
        "note") or entry.get("summary") or ""

    return {
        "name": name,
        "keywords": keywords,
        "trend_label": trend_label or "ë³´í•©",
        "change_pct": change_pct,
        "insight": insight.strip(),
    }


def _parse_change_value(value: Any) -> Optional[float]:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        match = re.search(r"-?\d+(?:\.\d+)?", value.replace(',', ''))
        if match:
            number = float(match.group(0))
            return number
    return None


def _create_cluster_fallback(
    keyword: str,
    hints: List[str],
    naver_metrics: Dict[str, Any],
) -> List[Dict[str, Any]]:
    if not naver_metrics.get("has_data"):
        return []

    terms = [term for term in hints if term]
    defaults = [
        f"{keyword} êµ¬ë§¤",
        f"{keyword} ê°€ê²©",
        f"{keyword} í›„ê¸°",
        f"{keyword} ë ˆì‹œí”¼",
        f"{keyword} ê¸°ê¸°",
        f"{keyword} ì´ë²¤íŠ¸",
    ]
    for default_term in defaults:
        if default_term not in terms:
            terms.append(default_term)

    while len(terms) < 6:
        terms.append(f"{keyword} ê´€ë ¨ {len(terms)}")

    momentum = _safe_float(naver_metrics.get("momentum_pct"))
    growth = _safe_float(naver_metrics.get("growth_pct"))
    latest = _safe_float(naver_metrics.get("latest_value"))

    clusters = [
        {
            "name": "í•µì‹¬ ìˆ˜ìš”",
            "keywords": [keyword, terms[0], terms[1]],
            "trend_label": _momentum_label(momentum),
            "change_pct": momentum,
            "insight": "í•µì‹¬ ê²€ìƒ‰ì–´ì™€ ì§ì ‘ì ì¸ ì—°ê´€ì–´ì—ì„œ ë‚˜íƒ€ë‚œ ìˆ˜ìš” íë¦„ì…ë‹ˆë‹¤.",
        },
        {
            "name": "êµ¬ë§¤ ê³ ë ¤ ë° ê°€ê²©",
            "keywords": [terms[2], terms[3]],
            "trend_label": _momentum_label(growth if growth else momentum / 2),
            "change_pct": growth if growth else momentum / 2,
            "insight": "êµ¬ë§¤ ì˜ë„ì™€ ê°€ê²© íƒìƒ‰ í‚¤ì›Œë“œì˜ ë³€í™”ë¥¼ í†µí•´ ì „í™˜ ê°€ëŠ¥ì„±ì„ íŒŒì•…í•˜ì„¸ìš”.",
        },
        {
            "name": "ê´€ë ¨ ë¼ì´í”„ìŠ¤íƒ€ì¼",
            "keywords": [terms[4], terms[5]],
            "trend_label": _momentum_label((momentum + growth) / 2 if (momentum or growth) else latest / 100),
            "change_pct": (momentum + growth) / 2 if (momentum or growth) else None,
            "insight": "ì½˜í…ì¸ /ë¼ì´í”„ìŠ¤íƒ€ì¼ í‚¤ì›Œë“œì˜ ë³€í™”ë¥¼ í™œìš©í•´ ìº í˜ì¸ ì†Œì¬ë¥¼ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        },
    ]

    return clusters


def _safe_float(value: Any) -> float:
    try:
        if value is None:
            return 0.0
        return float(value)
    except (TypeError, ValueError):
        return 0.0
